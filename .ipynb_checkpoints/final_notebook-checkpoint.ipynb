{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data and model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text from our medical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf vectors\n",
    "fulldocs = pd.read_csv(\"fulldocs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>docid.1</th>\n",
       "      <th>caseid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Document Text: Example 1\\nReferring Doctor: Un...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Document Text: Example 2\\nProgress Notes\\nDate...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Document Text: Example 3\\nChief Complaint:\\n1....</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Document Text: 14\\nse\\nLAIDA\\n*\\nPatient Infor...</td>\n",
       "      <td>4</td>\n",
       "      <td>11594</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Document Text: The first set of documents behi...</td>\n",
       "      <td>5</td>\n",
       "      <td>11594</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>145.0</td>\n",
       "      <td>Document Text: FAX COVER SHEET\\nDATE:\\nTO:\\nPH...</td>\n",
       "      <td>145</td>\n",
       "      <td>207766</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>146.0</td>\n",
       "      <td>Document Text: Page 1/20\\nPatient Profile\\nRes...</td>\n",
       "      <td>146</td>\n",
       "      <td>207813</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>147.0</td>\n",
       "      <td>Document Text: A SPECIALTY INFUSION COMPANY\\n....</td>\n",
       "      <td>147</td>\n",
       "      <td>207873</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>129.0</td>\n",
       "      <td>Document Text: PM\\nFROM: Fax\\nPAGE: 001 OF 021...</td>\n",
       "      <td>129</td>\n",
       "      <td>206600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>130.0</td>\n",
       "      <td>Document Text: PAGE: 001 OF 009\\nFAX SHEET\\nNu...</td>\n",
       "      <td>130</td>\n",
       "      <td>206600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     docid                                           fulltext  docid.1  \\\n",
       "0      1.0  Document Text: Example 1\\nReferring Doctor: Un...        1   \n",
       "1      2.0  Document Text: Example 2\\nProgress Notes\\nDate...        2   \n",
       "2      3.0  Document Text: Example 3\\nChief Complaint:\\n1....        3   \n",
       "3      4.0  Document Text: 14\\nse\\nLAIDA\\n*\\nPatient Infor...        4   \n",
       "4      5.0  Document Text: The first set of documents behi...        5   \n",
       "..     ...                                                ...      ...   \n",
       "142  145.0  Document Text: FAX COVER SHEET\\nDATE:\\nTO:\\nPH...      145   \n",
       "143  146.0  Document Text: Page 1/20\\nPatient Profile\\nRes...      146   \n",
       "144  147.0  Document Text: A SPECIALTY INFUSION COMPANY\\n....      147   \n",
       "145  129.0  Document Text: PM\\nFROM: Fax\\nPAGE: 001 OF 021...      129   \n",
       "146  130.0  Document Text: PAGE: 001 OF 009\\nFAX SHEET\\nNu...      130   \n",
       "\n",
       "     caseid  label  \n",
       "0        -1    1.0  \n",
       "1        -1    1.0  \n",
       "2        -1    4.0  \n",
       "3     11594    1.0  \n",
       "4     11594    1.0  \n",
       "..      ...    ...  \n",
       "142  207766    2.0  \n",
       "143  207813    2.0  \n",
       "144  207873    2.0  \n",
       "145  206600    1.0  \n",
       "146  206600    1.0  \n",
       "\n",
       "[147 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('important_words.txt', 'r', encoding=\"utf8\") as file:\n",
    "  data = file.read()\n",
    "  words = data.split('\\n')\n",
    "  words = list(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {}\n",
    "corpus = []\n",
    "tags = []\n",
    "for i, row in fulldocs.iterrows():\n",
    "  corpus.append(row['fulltext'])\n",
    "  tags.append(row['label'])\n",
    "  corpus_dict[i] = (row['fulltext'], row['label'])\n",
    "\n",
    "corpus_np = np.array(corpus)\n",
    "tags_np = np.array(tags)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "test_docs = vectorizer.fit_transform(corpus)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "final_vocab = []\n",
    "\n",
    "# get overlapping words with relevant words and word vectors made from the document\n",
    "for vocab in vocabulary:\n",
    "  splits = vocab.split()\n",
    "  for w in words:\n",
    "    if w in splits:\n",
    "      final_vocab.append(vocab)\n",
    "      break\n",
    "\n",
    "for i in range(len(tags)):\n",
    "  if tags[i] == 4:\n",
    "    tags[i] = 3.0\n",
    "  elif tags[i] == 0:\n",
    "    tags[i] = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our logistic regression using k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=1) # using 6 kfolds\n",
    "curr_train = []\n",
    "curr_test = []\n",
    "curr_f1_train = []\n",
    "curr_f1_test = []\n",
    "curr_f1_macro_train = []\n",
    "curr_f1_macro_test = []\n",
    "    \n",
    "for train_index, test_index in skf.split(corpus, tags):\n",
    "    # split corpus into train and test\n",
    "    features_train, features_test = corpus_np[train_index], corpus_np[test_index]\n",
    "    # split tags into train and test\n",
    "    labels_train, labels_test = tags_np[train_index], tags_np[test_index]\n",
    "    # train tfidf vectorizer\n",
    "    features_train = vectorizer.fit_transform(features_train)\n",
    "    features_test = vectorizer.transform(features_test)\n",
    "\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    final_vocab = []\n",
    "\n",
    "    # get overlapping words with relevant words and word vectors made from the document\n",
    "    for vocab in vocabulary:\n",
    "        splits = vocab.split()\n",
    "        for w in words:\n",
    "            if w in splits:\n",
    "                final_vocab.append(vocab)\n",
    "                break\n",
    "\n",
    "    # get the train vector matrix\n",
    "    train = pd.DataFrame(data=features_train.toarray(), columns=vocabulary)\n",
    "\n",
    "    # get the test vector matrix\n",
    "    test = pd.DataFrame(data=features_test.toarray(), columns=vocabulary)\n",
    "\n",
    "    curr_model = LogisticRegression(penalty = 'l1', solver = 'saga', C=10).fit(train[final_vocab], labels_train)\n",
    "\n",
    "    labels_pred = curr_model.predict(test[final_vocab])\n",
    "    labels_train_pred = curr_model.predict(train[final_vocab])\n",
    "\n",
    "    curr_train.append(accuracy_score(labels_train, labels_train_pred))\n",
    "    curr_test.append(accuracy_score(labels_test, labels_pred))\n",
    "\n",
    "    curr_f1_train.append(f1_score(labels_train, labels_train_pred, average='weighted'))\n",
    "    curr_f1_test.append(f1_score(labels_test, labels_pred, average='weighted'))\n",
    "\n",
    "    curr_f1_macro_train.append(f1_score(labels_train, labels_train_pred, average='macro'))\n",
    "    curr_f1_macro_test.append(f1_score(labels_test, labels_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out accuracy scores and f1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error:  0.5468701408325559\n",
      "Test error:  0.4694444444444445\n",
      "Train weighted f1:  0.43253392842807736\n",
      "Test weighted f1:  0.31457743457743453\n",
      "Train macro f1:  0.2858784510300954\n",
      "Test macro f1:  0.1336136136136136\n"
     ]
    }
   ],
   "source": [
    "print(\"Train error: \", np.mean(curr_train))\n",
    "print(\"Test error: \", np.mean(curr_test))\n",
    "print(\"Train weighted f1: \", np.mean(curr_f1_train))\n",
    "print(\"Test weighted f1: \", np.mean(curr_f1_test))\n",
    "print(\"Train macro f1: \", np.mean(curr_f1_macro_train))\n",
    "print(\"Test macro f1: \", np.mean(curr_f1_macro_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
